{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cabbef9-aeca-4ce9-9967-7e75bcd33082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['第二部分 架构师的视角\\n', '1 访问远程服务\\n', '1.1 远程服务调用\\n', '进程间通信（IPC）的解决方法\\n', '管道/具名管道：用于解决进程间传递少量字符流或字节流\\n', '信号：用于通知目标进程有某种事件发生\\n', '信号量：用于在两个进程之间同步协作\\n', '消息队列：用于进程间传递数据量较多的通信\\n', '共享内存（效率最高）：运行多个进程访问同一块公共内存空间\\n', '本地套接字接口：用于不同机器之间的进程通信\\n', '1.2 三个基本问题\\n', '如何表示数据：序列化和反序列化\\n', '如何传递数据：Wire Protocol，传输协议\\n', '如何表示方法：接口描述语言（Android接口定义语言、Web服务描述语言、JSON Web服务协议）\\n', '1.3 RPC的发展\\n', '面向对象：RMI、.NET Remoting\\n', '性能：gRPC（支持多路复用和Header压缩）、Thrift（基于传输层的TCP协议）\\n', '简化：JSON-RPC\\n', '1.4 REST设计风格\\n', '术语定义\\n', '\\n', '资源\\n', '表征：不同的形式\\n', '状态：在特定语境中的上下文信息\\n', '转移：服务端将资源表征从一个状态转移到另一个状态\\n', '统一接口：GET、HEAD、POST、PUT、DELETE、TRACE、OPTION\\n', '超文本驱动：通过超文本内部的链接进行跳转\\n', '自描述信息：Content-Type\\n', 'RESTful系统特点\\n', '\\n', '客户端与服务端分离：用户界面所关注的逻辑和数据存储的逻辑分离\\n', '无状态：每次从客户端发送的请求中，仅包含必要的上下文信息\\n', '可缓存：将部分客户端的应答缓存\\n', '分层系统：客户端一般不需要知道是否直接连接到最终的服务器（透明访问）\\n', '统一接口：面向资源编程\\n', '按需代码：将可执行的软件程序从服务端发送到客户端，WebAssembly\\n', 'RMM（Richardson成熟度模型）\\n', '\\n', '第0级：完全不REST\\n', '第1级：开始引入资源的概念，使用资源ID进行请求\\n', '第2级：引入统一接口，映射到HTTP协议的方法上，把不同业务需求抽象为对资源的增删改等操作\\n', '第3级：超媒体控制，请求回应能够描述清楚后续可能发生的状态转移\\n', '不足与争议\\n', '\\n', '面向资源的编程思想只适合做CRUD，面向过程、面向对象编程才能处理真正复杂的业务逻辑\\n', 'REST与HTTP完全绑定，不适合应用于要求高性能传输的场景中\\n', 'REST不利于事务支持\\n', 'REST没有传输可靠性支持\\n', 'REST缺乏对资源进行“部分”和“批量”处理的能力\\n', '2 事务处理\\n', '2.1 事务特性（刚性事务）\\n', '原子性（Atomic）']\n",
      "{'<PAD>': 0, '第二': 1, '部分': 2, ' ': 3, '架构师': 4, '的': 5, '视角': 6, '\\n': 7, '1': 8, '访问': 9, '远程': 10, '服务': 11, '1.1': 12, '调用': 13, '进程': 14, '间通信': 15, '（': 16, 'IPC': 17, '）': 18, '解决': 19, '方法': 20, '管道': 21, '/': 22, '具名': 23, '：': 24, '用于': 25, '间': 26, '传递': 27, '少量': 28, '字符': 29, '流': 30, '或': 31, '字节': 32, '信号': 33, '通知': 34, '目标': 35, '有': 36, '某种': 37, '事件': 38, '发生': 39, '信号量': 40, '在': 41, '两个': 42, '之间': 43, '同步': 44, '协作': 45, '消息': 46, '队列': 47, '传递数据': 48, '量': 49, '较': 50, '多': 51, '通信': 52, '共享内存': 53, '效率': 54, '最高': 55, '运行': 56, '多个': 57, '同': 58, '一块': 59, '公共': 60, '内存空间': 61, '本地': 62, '套': 63, '接字': 64, '接口': 65, '不同': 66, '机器': 67, '1.2': 68, '三个': 69, '基本': 70, '问题': 71, '如何': 72, '表示': 73, '数据': 74, '序列化': 75, '和': 76, '反': 77, 'Wire': 78, 'Protocol': 79, '，': 80, '传输': 81, '协议': 82, '描述语言': 83, 'Android': 84, '接口定义': 85, '语言': 86, '、': 87, 'Web': 88, 'JSON': 89, '1.3': 90, 'RPC': 91, '发展': 92, '面向对象': 93, 'RMI': 94, '.': 95, 'NET': 96, 'Remoting': 97, '性能': 98, 'gRPC': 99, '支持': 100, '多路复用': 101, 'Header': 102, '压缩': 103, 'Thrift': 104, '基于': 105, '传输层': 106, 'TCP': 107, '简化': 108, '-': 109, '1.4': 110, 'REST': 111, '设计': 112, '风格': 113, '术语': 114, '定义': 115, '资源': 116, '表征': 117, '形式': 118, '状态': 119, '特定': 120, '语境': 121, '中': 122, '上下文': 123, '信息': 124, '转移': 125, '服务端': 126, '将': 127, '从': 128, '一个': 129, '到': 130, '另': 131, '统一': 132, 'GET': 133, 'HEAD': 134, 'POST': 135, 'PUT': 136, 'DELETE': 137, 'TRACE': 138, 'OPTION': 139, '超文本': 140, '驱动': 141, '通过': 142, '内部': 143, '链接': 144, '进行': 145, '跳转': 146, '自描述': 147, 'Content': 148, 'Type': 149, 'RESTful': 150, '系统': 151, '特点': 152, '客户端': 153, '与': 154, '分离': 155, '用户界面': 156, '所': 157, '关注': 158, '逻辑': 159, '存储': 160, '无': 161, '每次': 162, '发送': 163, '请求': 164, '仅': 165, '包含': 166, '必要': 167, '可': 168, '缓存': 169, '应答': 170, '分层': 171, '一般': 172, '不': 173, '需要': 174, '知道': 175, '是否': 176, '直接': 177, '连接': 178, '最终': 179, '服务器': 180, '透明': 181, '面向': 182, '编程': 183, '按': 184, '需': 185, '代码': 186, '执行': 187, '软件程序': 188, '发送到': 189, 'WebAssembly': 190, 'RMM': 191, 'Richardson': 192, '成熟度': 193, '模型': 194, '第': 195, '0': 196, '级': 197, '完全': 198, '开始': 199, '引入': 200, '概念': 201, '使用': 202, 'ID': 203, '2': 204, '映射': 205, 'HTTP': 206, '上': 207, '把': 208, '业务': 209, '需求': 210, '抽象': 211, '为': 212, '对': 213, '增删': 214, '改等': 215, '操作': 216, '3': 217, '超媒体': 218, '控制': 219, '回应': 220, '能够': 221, '描述': 222, '清楚': 223, '后续': 224, '可能': 225, '不足': 226, '争议': 227, '思想': 228, '只': 229, '适合': 230, '做': 231, 'CRUD': 232, '过程': 233, '面向对象编程': 234, '才能': 235, '处理': 236, '真正': 237, '复杂': 238, '绑定': 239, '应用': 240, '于': 241, '要求': 242, '高性能': 243, '场景': 244, '不利于': 245, '事务': 246, '没有': 247, '可靠性': 248, '缺乏': 249, '“': 250, '”': 251, '批量': 252, '能力': 253, '事务处理': 254, '2.1': 255, '特性': 256, '刚性': 257, '原子': 258, '性': 259, 'Atomic': 260}\n",
      "indices: [1, 2, 3, 4, 5, 6, 7]\n",
      "(tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 4, 5, 6, 7]))\n",
      "indices: [8, 3, 9, 10, 11, 7]\n",
      "(tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  8,  3,  9,\n",
      "        10, 11]), tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  9, 10,\n",
      "        11,  7]))\n",
      "indices: [1, 2, 3, 4, 5, 6, 7]\n",
      "indices: [7]\n",
      "indices: [195, 204, 197, 24, 200, 132, 65, 80, 205, 130, 206, 82, 5, 20, 207, 80, 208, 66, 209, 210, 211, 212, 213, 116, 5, 214, 215, 216, 7]\n",
      "indices: [195, 217, 197, 24, 218, 219, 80, 164, 220, 221, 222, 223, 224, 225, 39, 5, 119, 125, 7]\n",
      "indices: [132, 65, 24, 182, 116, 183, 7]\n",
      "indices: [33, 24, 25, 34, 35, 14, 36, 37, 38, 39, 7]\n",
      "indices: [53, 16, 54, 55, 18, 24, 56, 57, 14, 9, 58, 59, 60, 61, 7]\n",
      "indices: [72, 73, 20, 24, 65, 83, 16, 84, 85, 86, 87, 88, 11, 83, 87, 89, 3, 88, 11, 82, 18, 7]\n",
      "indices: [46, 47, 24, 25, 14, 26, 48, 49, 50, 51, 5, 52, 7]\n",
      "indices: [111, 245, 246, 100, 7]\n",
      "indices: [8, 3, 9, 10, 11, 7]\n",
      "indices: [7]\n",
      "indices: [161, 119, 24, 162, 128, 153, 163, 5, 164, 122, 80, 165, 166, 167, 5, 123, 124, 7]\n",
      "indices: [7]\n",
      "indices: [195, 8, 197, 24, 199, 200, 116, 5, 201, 80, 202, 116, 203, 145, 164, 7]\n",
      "indices: [111, 249, 213, 116, 145, 250, 2, 251, 76, 250, 252, 251, 236, 5, 253, 7]\n",
      "indices: [93, 24, 94, 87, 95, 96, 3, 97, 7]\n",
      "indices: [40, 24, 25, 41, 42, 14, 43, 44, 45, 7]\n",
      "indices: [171, 151, 24, 153, 172, 173, 174, 175, 176, 177, 178, 130, 179, 5, 180, 16, 181, 9, 18, 7]\n",
      "indices: [255, 3, 246, 256, 16, 257, 246, 18, 7]\n",
      "indices: [98, 24, 99, 16, 100, 101, 76, 102, 103, 18, 87, 104, 16, 105, 106, 5, 107, 82, 18, 7]\n",
      "indices: [150, 151, 152, 7]\n",
      "indices: [7]\n",
      "indices: [72, 48, 24, 78, 3, 79, 80, 81, 82, 7]\n",
      "indices: [226, 154, 227, 7]\n",
      "indices: [114, 115, 7]\n",
      "indices: [68, 3, 69, 70, 71, 7]\n",
      "indices: [14, 15, 16, 17, 18, 5, 19, 20, 7]\n",
      "indices: [182, 116, 5, 183, 228, 229, 230, 231, 232, 80, 182, 233, 87, 234, 235, 236, 237, 238, 5, 209, 159, 7]\n",
      "indices: [12, 3, 10, 11, 13, 7]\n",
      "indices: [62, 63, 64, 65, 24, 25, 66, 67, 43, 5, 14, 52, 7]\n",
      "indices: [258, 259, 16, 260, 18]\n",
      "input: torch.Size([32, 20])\n",
      "init h_prev: torch.Size([32, 256])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 99\u001b[0m\n\u001b[1;32m     97\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     98\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 99\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 72\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     70\u001b[0m h_prev \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minit_hidden(inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit h_prev:\u001b[39m\u001b[38;5;124m'\u001b[39m,h_prev\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 72\u001b[0m output, h_prev \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_prev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m,output\u001b[38;5;241m.\u001b[39msize(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m,model\u001b[38;5;241m.\u001b[39membedding(targets[:,\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     74\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, model\u001b[38;5;241m.\u001b[39membedding(targets))\n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import jieba  # 用于中文分词\n",
    "\n",
    "# 读取周杰伦歌词文件，假设文件名为jay_chou_lyrics.txt，每行是一首歌词\n",
    "def read_lyrics(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lyrics = f.readlines()\n",
    "    return lyrics\n",
    "\n",
    "# 构建数据集类\n",
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, lyrics, word2idx, seq_length):\n",
    "        self.lyrics = lyrics\n",
    "        self.word2idx = word2idx\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = len(word2idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lyrics)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lyric = self.lyrics[idx]\n",
    "        words = jieba.lcut(lyric) # 分词。输出分词后的列表\n",
    "        # 将歌词转换为索引序列\n",
    "        indices = [self.word2idx.get(word, 0) for word in words]\n",
    "        print('indices:',indices)\n",
    "        # 生成输入和目标序列\n",
    "        inputs = indices[:-1]\n",
    "        targets = indices[1:]\n",
    "\n",
    "        # 对输入序列进行填充或截断\n",
    "        if len(inputs) < self.seq_length:\n",
    "            inputs = [0] * (self.seq_length - len(inputs)) + inputs\n",
    "        else:\n",
    "            inputs = inputs[-self.seq_length:]\n",
    "        inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "\n",
    "        # 对目标序列进行填充或截断\n",
    "        if len(targets) < self.seq_length:\n",
    "            targets = [0] * (self.seq_length - len(targets)) + targets\n",
    "        else:\n",
    "            targets = targets[-self.seq_length:]\n",
    "        targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "# 构建词表\n",
    "def build_vocab(lyrics):\n",
    "    word2idx = {\"<PAD>\": 0}\n",
    "    idx = 1\n",
    "    for lyric in lyrics:\n",
    "        words = jieba.lcut(lyric)\n",
    "        for word in words:\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = idx\n",
    "                idx += 1\n",
    "    return word2idx\n",
    "\n",
    "# 训练函数\n",
    "def train(model, dataloader, optimizer, criterion, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in dataloader:\n",
    "            print('input:',inputs.size())\n",
    "            optimizer.zero_grad()\n",
    "            h_prev = model.init_hidden(inputs.size(0))\n",
    "            print('init h_prev:',h_prev.size())\n",
    "            output, h_prev = model(inputs, h_prev)\n",
    "            print('output',output.size(),'targets',model.embedding(targets[:,0]).size())\n",
    "            loss = criterion(output, model.embedding(targets))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# 主函数\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/home/ma-user/work/dev/easy-nlp-main/docs/chapter1/test.txt\"\n",
    "    lyrics = read_lyrics(file_path)\n",
    "    print(lyrics)\n",
    "    word2idx = build_vocab(lyrics)\n",
    "    print(word2idx)\n",
    "    seq_length = 20\n",
    "    dataset = LyricsDataset(lyrics, word2idx, seq_length)\n",
    "    print(dataset[0])\n",
    "    print(dataset[1])\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    vocab_size = dataset.vocab_size\n",
    "    hidden_size = 256\n",
    "    num_layers = 2\n",
    "    output_size = vocab_size\n",
    "    model = CustomRNN(vocab_size, hidden_size, num_layers, output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train(model, dataloader, optimizer, criterion, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "780ed310-3ab5-44e2-89ce-74b0ba4de1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting jieba\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=cb1778a85057dd0ed71a1cc72dea68c8831b9ea18506d6544669bfdb061d3aa3\n",
      "  Stored in directory: /home/ma-user/.cache/pip/wheels/2d/22/9e/9af7e8c2773513ac75905acfb75073922bcc1aa176f730a0c9\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-2.0.0/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46dcb016-3bf7-49e0-abe6-266458d61c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # 取最后一个时间步的输出\n",
    "        return out\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros( batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c35a5a7-489c-42f3-bd6c-28bbfc40889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers=1, batch_first=True):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        super().__init__()\n",
    "        self.W_ih = nn.Parameter(torch.rand(self.input_size, self.hidden_size))\n",
    "        self.W_hh = nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.b_ih = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        self.b_hh = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        \n",
    "    def forward(self,x_t,h_prev=None):\n",
    "        if h_prev == None:\n",
    "             h_prev = torch.zeros( x_t.size(0), self.hidden_size)\n",
    "        output = torch.tanh(torch.matmul(x_t, self.W_ih) + self.b_ih + torch.matmul(h_prev, self.W_hh) + self.b_hh)\n",
    "        return output,output[:,-1,:].unsqueeze(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d78fd837-be48-4f36-b2b5-98a8fc2f9db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.9024,  0.9291,  0.7738],\n",
      "         [-0.2708,  0.5224,  0.6002],\n",
      "         [-0.9924, -0.9147, -0.8783],\n",
      "         [-0.3688,  0.0709, -0.6016],\n",
      "         [ 0.4797, -0.0872, -0.5128]]], grad_fn=<TanhBackward0>)\n",
      "torch.Size([1, 5, 3])\n",
      "tensor([[[ 0.4797, -0.0872, -0.5128]]], grad_fn=<UnsqueezeBackward0>)\n",
      "torch.Size([1, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# 单向、单层rnn\n",
    "single_rnn = RNNLayer(input_size=4, hidden_size=3, num_layers=1, batch_first=True) # batch_first=True表示输入数据的维度为[batch_size, seq_len, input_size]\n",
    "input = torch.randn(1, 5, 4) # 输入数据维度为[batch_size, seq_len, input_size]\n",
    "output,h_n = single_rnn(input) # output维度为[batch_size, seq_len, hidden_size=3]，h_n维度为[num_layers=1, batch_size, hidden_size=3]\n",
    "print(output, output.shape, h_n, h_n.shape,  sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "615b30a1-0bac-4204-9ebb-6b121a32e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义 RNN 模型\n",
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, output_size):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.W_ih = nn.Parameter(torch.rand(hidden_size, hidden_size))\n",
    "        self.W_hh = nn.Parameter(torch.rand(hidden_size, hidden_size))\n",
    "        self.b_ih = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.b_hh = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        print('x',x.size())\n",
    "        batch_size, seq_length = x.size()\n",
    "        embedded = self.embedding(x)\n",
    "        print('embedded',embedded.size())\n",
    "        hiddens = []\n",
    "        for t in range(seq_length):\n",
    "            x_t = embedded[:, t, :]\n",
    "            print(x_t.size(),self.W_ih.size(),self.b_ih.size() ,h_prev.size(), self.W_hh.size(),self.b_hh.size())\n",
    "            h_t = torch.tanh(torch.mm(x_t, self.W_ih) + self.b_ih + torch.mm(h_prev, self.W_hh) + self.b_hh)\n",
    "            hiddens.append(h_t)\n",
    "            h_prev = h_t\n",
    "        \n",
    "        h_final = hiddens[-1]\n",
    "        print('h_final',h_final.size())\n",
    "        output = self.fc(h_final)\n",
    "        return output, h_prev\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros( batch_size, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e3f4a-ee2f-4fba-ad23-8ad424d7670f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cfae734d-bba3-4c8b-9ec1-848b4ff2f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.W_ih = nn.Parameter(torch.rand(self.input_size, self.hidden_size))\n",
    "        self.W_hh = nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.b_ih = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        self.b_hh = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        self.fc = nn.Linear(self.hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        print(x.size())\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        hiddens = []\n",
    "        for t in range(seq_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h_t = torch.tanh(torch.mm(x_t, self.W_ih) + self.b_ih + torch.mm(h_prev, self.W_hh) + self.b_hh)\n",
    "            hiddens.append(h_t)\n",
    "            h_prev = h_t\n",
    "        h_final = hiddens[-1]\n",
    "        print('h_final',h_final.size())\n",
    "        output = self.fc(h_final)\n",
    "        return output, h_prev\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "13e2a8fc-b484-4a65-9c3d-760a91b4ab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init h_prev: torch.Size([2, 256])\n",
      "torch.Size([2, 5, 100])\n",
      "h_final torch.Size([2, 256])\n",
      "output torch.Size([2, 100])\n",
      "torch.Size([2, 100])\n",
      "Epoch 0, Loss: 1.334328055381775\n",
      "init h_prev: torch.Size([2, 256])\n",
      "torch.Size([2, 5, 100])\n",
      "h_final torch.Size([2, 256])\n",
      "output torch.Size([2, 100])\n",
      "torch.Size([2, 100])\n",
      "Epoch 1, Loss: 1.228675365447998\n",
      "init h_prev: torch.Size([2, 256])\n",
      "torch.Size([2, 5, 100])\n",
      "h_final torch.Size([2, 256])\n",
      "output torch.Size([2, 100])\n",
      "torch.Size([2, 100])\n",
      "Epoch 2, Loss: 0.9169263243675232\n",
      "init h_prev: torch.Size([2, 256])\n",
      "torch.Size([2, 5, 100])\n",
      "h_final torch.Size([2, 256])\n",
      "output torch.Size([2, 100])\n",
      "torch.Size([2, 100])\n",
      "Epoch 3, Loss: 1.086367130279541\n",
      "init h_prev: torch.Size([2, 256])\n",
      "torch.Size([2, 5, 100])\n",
      "h_final torch.Size([2, 256])\n",
      "output torch.Size([2, 100])\n",
      "torch.Size([2, 100])\n",
      "Epoch 4, Loss: 1.2416033744812012\n",
      "init h_prev: torch.Size([2, 256])\n",
      "torch.Size([2, 5, 100])\n",
      "h_final torch.Size([2, 256])\n",
      "output torch.Size([2, 100])\n",
      "torch.Size([2, 100])\n",
      "Epoch 5, Loss: 0.909860372543335\n",
      "init h_prev: torch.Size([2, 256])\n",
      "torch.Size([2, 5, 100])\n",
      "h_final torch.Size([2, 256])\n",
      "output torch.Size([2, 100])\n",
      "torch.Size([2, 100])\n",
      "Epoch 6, Loss: 1.2592588663101196\n",
      "init h_prev: torch.Size([2, 256])\n",
      "torch.Size([2, 5, 100])\n",
      "h_final torch.Size([2, 256])\n",
      "output torch.Size([2, 100])\n",
      "torch.Size([2, 100])\n",
      "Epoch 7, Loss: 1.4095985889434814\n",
      "init h_prev: torch.Size([2, 256])\n",
      "torch.Size([2, 5, 100])\n",
      "h_final torch.Size([2, 256])\n",
      "output torch.Size([2, 100])\n",
      "torch.Size([2, 100])\n",
      "Epoch 8, Loss: 1.0754644870758057\n",
      "init h_prev: torch.Size([2, 256])\n",
      "torch.Size([2, 5, 100])\n",
      "h_final torch.Size([2, 256])\n",
      "output torch.Size([2, 100])\n",
      "torch.Size([2, 100])\n",
      "Epoch 9, Loss: 1.2917191982269287\n"
     ]
    }
   ],
   "source": [
    "# 假设输入歌词维度、隐藏层维度、层数、输出维度等\n",
    "input_size = 100\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "output_size = 100\n",
    "rnn = CustomRNN(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# 模拟输入数据（实际要根据歌词进行词向量等转换），这里假设一批次2条数据，序列长度5，维度为input_size\n",
    "x = torch.randn(2, 5, input_size)\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    h_prev = rnn.init_hidden(2)\n",
    "    print('init h_prev:',h_prev.size())\n",
    "    output, h_prev = rnn(x, h_prev)\n",
    "    print('output',output.size())\n",
    "    print(torch.randn(2, output_size).size())\n",
    "    loss = criterion(output, torch.randn(2, output_size))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f9828-24a8-4ec8-800a-bdfae3b736cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.0",
   "language": "python",
   "name": "pytorch-2.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
